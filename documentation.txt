The intent of this framework is to be simple and easy to get started for simple projects, while still having enough options and extensibility to grow for more complex projects. To get started you truly only need to populate one table, which is "etl.Task". All other tables provide additional optional functionality.

Initial deployment:
You will need the following set up in your Azure subscription:
- Key Vault
- an empty Azure SQL Database to host the configuration information
- an empty Azure Data Factory
    - do not set up the self-hosted integration runtime
    - do not set up source code integration
- an Azure data lake v2 storage account (hierarchical storage enabled)
- Unless all your source systems can be reached by the autoresolveintegrationruntime, you need an on-prem box with the following installed
    - We'll walk you through some of the integration runtime setup bits, so don't jump ahead!
    - Java Runtime Environment (x64) - please double-check that you have the x64 version!
    - Visual C++ 2010 redistributable (x64)

Setting up Key Vault
- Ensure that your Data Factory has Get and List permissions
- set up a secret named kv-etlcontrol-connstr with the connection string (including user name and password) for your configuration Database
- set up a secret named kv-datalake-key with the access key to your adlsv2 account
- you can hold off setting up the connection strings for other sources until later.

Setting up Data Factory (for dev work)
- in Data Factory, create an integration runtime named ir-selfhosted-onprem
    - install on the server, etc.
    - click on the code button next to ir-selfhosted-onprem and save the JSON for the integration runtime.
- Open up your git tool of choice
    - Open up BG-Commit\DataLakeHydrator and create a new branch
    - download the new branch to someplace handy with your Git tool of choice
- In a text editor (I suggest Visual Studio Code) make a few changes to the linked services JSON files:
    - change the URL in the ls_adlsv2_autoresolveintegrationruntime and ls_adlsv2_selfhosted linked services to your storage account
    - change the URL in the ls_keyvault to your keyvault account
    - in ir-selfhosted-onprem.json, paste in the code that you saved earlier for YOUR integration runtime.
- Commit and push the changes to BG-Commit
- Connect your Data Factory to BG-Commit\DataLakeHydrator.
    Github account: BG-Commit
    root folder: /data Factory
    Collaboration branch: the branch you made
- Set up the control database and test things out.

Setting up the control Database
- You can either use schema compare to deploy the structure OR use a bacpac file.  Just get the structure applied to it.
- Then you can start configuring etl.Tasks, etl.Sources and other stuff

Tasks:
The "etl.Task" table contains a row for each table you want to import. The minimum fields you should enter are (more or less):

SourceName: the name you've decided to give the source system. It assumes that the source system is handled specifically in ADF unless you add an entry to etl.Sources
SourceTableName: name of the table you want to import
TargetDataLakeContainer: the name of the storage account container you want to save the parquet file to
TargetDataLakeFolder: destination folder
TargetDataLakeFileName: this is optional. if you don't supply, then the ETL will use the name of the table as the root of the file name.

All the other columns in that table add additional functionality, such as custom queries, incremental data-loading functionality, and the concept of "advanced" tasks. Advanced tasks allow you to more heavily customize the ETL while still retaining some roots in this "common" framework. That's all I'll say here since this is supposed to be brief.

Jobs:

Jobs are an orchestration concept and are optional within the framework. Jobs are simply a group of one or more tasks. They exist because most projects eventually grow to need something like them. They are made optional for the sake of simplicity. In truth, this is a baked-in, implicit job which all tasks belong to. This is job is named "default". You don't need to set this job up in the "etl.Job" table. Think of it as an virtual or imaginary job. As a sort of reference to imaginary numbers, it has an imaginary JobKey of -1. Rather than simple this might all sound confusing, but what it means is that you simply add a handful of rows to the "etl.Task" table and then run the "default" job, and all your tasks will execute. Need to only execute a subset of your tasks? Congratulations, you have now grown to the point where setting up and executing different jobs will benefit you.

Operation overview:

A job, whether the default "virtual" job or an explicit job in the etl.Job table, is executed via the "pl_job_master" pipeline. This pipeline accepts a single parameter, which is the job name. Leave it blank to run the default job. The pipeline checks the etl.JobAudit table. This contains one record per each job execution. For restartability purposes, if the last execution of the job failed, then the pipeline will try to adopt the failed "JobAuditKey" and try again. Otherwise, if the prior run was successful, then the pipeline simply logs a new row to the table and grabs the newly-generated "JobAuditKey". It also assembles a list of all the tasks that need to execute as part of this job. It does this by joining to the etl.Task table via the etl.TaskJobBridge table. It finds all the TaskKeys that need to be executed and then seeds one row per each task in the etl.TaskAudit table. The etl.TaskAudit table retains a reference to the JobAudit table.

The etl.TaskAudit table was pre-seeded with tasks during the "pl_job_master" pipeline. The same pipeline iterates over each row. The "pl_taskaudit_launcher" pipeline is executed for each row. The pipeline does some basic logging but ultimately passes the work onto another pipeline, "pl_standard_ingest". 

The "pl_standard_ingest" pipeline takes a particular "TaskAuditKey" value as a parameter. It joins this back to the etl.Task table to get the info it needs to copy the data from the source system into a parquet file within the data lake. It adds the "TaskAuditKey" value to the parquet filename for traceability.

Sources:
This is used to override default behaviors for a source, allowing you to specify the type, authentication, custom keyvault secret names, user names and if you really want to get crazy, integration runtime.  Talk to Steve Cardella if you want to specify the integration runtimes.
Type - currently supported is SQLServer or AzureSQL.  Any other type would require you to make an addition to the switch statement and check constraint on etl.Sources.
Authentication - for SQLServer, it allows you to specify "SQL" for SQL authentication or "WINDOWS" for Windows authentication (Active directory, etc.)
UserName - If you are using windows authentication, specify the Domain\Username here.
IntegrationRuntimeName - If you're using multiple self-hosted integration runtimes, you specify which integration runtime you want here.  If you specify something here, you need extra entries in the switch statement and supporting datasets/linked services.
ConnectionStringSecret - Fill this in if you need to override the connection string secret name.  It is case sensitive!  The default is 'kv-'+SourceName+'-connstr'.
PasswordSecret - If you're using windows authentication, use this to override the password secret name.  It is case sensitive!  The default is 'kv-'+SourceName+'-passwd'.


Table descriptions:

etl.Job: one row per job
etl.JobAudit: history of each job execution
etl.KeyValueConfig: a two-column key-value config table for various system-wide settings
etl.Task: each discrete table/query/etc that needs to be ingested
etl.TaskAudit: history of ingestion detail for a particular task during a particular job execution. also serves as the "worklist" for the ETL to follow during a job execution
etl.TaskJobBridge: simple bridge table that maps tasks to jobs
etl.TaskLimitAudit: audit table detailing history of each time limits are updated on an incremental task
 